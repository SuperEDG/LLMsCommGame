# 技术分析：大语言模型在沟通游戏中的应用——“狼人杀”为例

## 1. 研究主题

### 1.1 应用总结

研究着眼于大语言模型（LLMs）在沟通游戏——“狼人杀”中的应用。作为不完全信息的游戏，狼人杀要求玩家通过语言沟通来收集和推断信息，以实现各自阵营的胜利目标。研究的核心是无需调整模型参数的情况下，利用回溯和反思先前的沟通经验来优化模型中在游戏中的表现。

### 1.2 游戏设定

狼人杀共七名玩家，分配五种不同角色：两狼人、两村民、一女巫、一守卫和一预言家。玩家在游戏起始阶段仅知晓自身角色。狼人阵营的目标是消灭所有平民，而平民阵营需在最后一轮保持至少一名平民存活的同时投出所有狼人。


## 2. 关键发现

### 2.1 解决的挑战

LLMs在不完全信息的游戏中的研究难点分为三点：**有限的上下文长度**，**需求复杂推理**，**从经验中学习**。

### 2.2 技术框架

- （1）无需调优的LLMs框架：对LLMs模型不进行微调，只构建Prompt的范式来加强模型对于狼人杀游戏的表现。
- （2）核心方法：[1] 采用了一种检索并反映关键历史信息的方法，为每个基于LLM的代理构建一个紧凑的上下文；[2] 提出了一种从过往经验中根据当前状况提取建议的机制，旨在防止LLMs在多轮游戏中重复相似的错误，利用的是先前的沟通、自身推理并结合评分构建的记忆库；[3] 利用思维链（CoT）进行推理，并结合Zero-shot方法，逐步引导模型按照既定逻辑推理出结果。

### 2.3 实验结果

实验显示，采用该框架的LLMs能够在不调整模型参数的前提下有效参与狼人杀游戏。实验过程中开始浮现出策略行为：信任、对抗、伪装和领导。


## 3. 技术细节

**框架需要的符号标记**
- t: 表示游戏中的天。
- r: 表示游戏的回合。
- i: 表示不同的玩家。
- G: 代理给出的回应
- O: 代理收到的观察。
- R: 代表代理基于历史沟通生成的总结。
- S: 表示从过去经验中提取的建议。
- E: 表示经验池，存储历史经验的集合。

### 3.1 提示词框架的构成

- **游戏规则和角色描述**：包括基本的游戏规则、角色能力与目标和一些基础策略。
- **最近的K条信息**：指的是代理在当前天最近接收的K条信息。
  - O: 代表最近的信息，如怀疑、猜测和自述。
  - V: 代表通过启发式选择的关键信息（角色披露，能力使用，投票意图）。
  - R: 代表基于O和V的反映或总结。
- **从过去的经验中提取的建议**：使用以往的经验来在当前轮次中指导决策。
- **思维链提示(CoT prompt)**：引导模型按照一定的逻辑推理过程生成响应。

### 3.2 收集历史信息

- **新鲜度**：将最近的K条信息（记为O）包含在上下文中，因为最新的信息通常是最相关的。
- **信息性**：通过规则匹配和启发式度量选择携带关键信息的信息并将其（记为V）包含在上下文中，例如披露代理角色的信息。
- **完整性**：生成一个反映（记为R）来总结整个沟通历史的关键信息。

### 3.3 从经验中学习

- **经验池**：一个存储所有代理在所有轮次的经验的集合。
- **得分系统**：一个旨在鼓励代理尽快赢得游戏或在无法赢得比赛的情况下尽可能慢地输掉比赛的系统。
- **建议提取**：基于当前的反映和得分，从经验池中提取相关的经验(基于Sentence-BERT的检索机制)，并生成一个建议，用于在后续的游戏决策中作为引导。

## 4. 实验部分

### 4.1 实验设置
   - **框架**：使用了Chatarena，一个允许连接多个大型语言模型（LLMs）的框架。
   - **模型**：选择了gpt-3.5-turbo-0301作为后端的LLMs。
   - **参数设置**：窗口大小K = 15(最近的信息)，预定义问题数量L = 5，自由提问数量M = 2，经验检索阈值epsilon = 0.85，最多保留50个经验等。
   - **其他设置**：对话顺序是随机确定的，LLM的温度被设置为以控制CoT推理和其他内容的生成（CoT为0，生成其他为0.3）。

### 4.2 经验池构建
   - **经验池大小**：分别用10、20、30 和 40轮游戏构建经验池以探讨其对性能的影响。
   - **角色分配**：每轮中，1到7号玩家的角色随机分配。
   - **经验池用途**：主要用于评估，而非训练模型。
   - **初步发现**：简单的基本人类先验（例如关于有效游戏策略的提示）在经验学习过程中充当了一种引导机制。

### 4.3 利用经验的分析
   - **建议生成**：代理生成的建议。
   - **效果评估**：使用胜率和平均持续时间衡量代理性能。
   - **关键发现**：从经验中学习通常可以提高村民方的胜率；然而，当经验量较大时，方法的有效性可能不稳定，并且胜率无明确趋势。**40轮后不如20轮**

### 4.4 消融研究
   - **定性分析**：考虑了移除单个组件（如游戏规则、近期消息、信息性消息等）对模型输出的影响。
   - **定量分析**：通过人工评估确定模型输出（移除特定组件后）的合理性。
   - **关键发现**：所有组件（游戏规则、近期消息、信息性消息、问题、反思、经验建议和CoT提示）在方法中都是必要的，每个部分都对生成更合理、更真实的响应起到了关键作用。


## 5. 出现的策略行为总结

**信任**：
   - **定义**：信任涉及到对其他玩家共享目标的信念，以及他们会按照这些目标行动的预期。
   - **实例**：玩家可能分享对自己不利的信息，或在某些情况下与其他玩家联合，指责某人为敌人。
   - **发现**：LLMs基于证据决定是否信任他人，展现出独立思考的能力。信任行为似乎会随着游戏的进行而增加，LLMs也会基于分析解散不合理的信任关系。
   
**对抗**
   - **定义**：对抗行为指代玩家为实现与对立阵营目标不符的目标而采取的行动。
   - **实例**：狼人夜间攻击其他玩家或白天指控某人是狼人，保护自己的行动也被视为对抗。
   - **发现**：LLMs可能采取隐性的对抗行为，例如，不仅仅跟随狼人的意图，而是基于自己的判断表达不同意。

 **伪装**
   - **定义**：伪装包括掩饰自己的身份和误导其他玩家的行动。
   - **实例**：狼人可能声称自己是村民，或者玩家可能编造实际上不存在的事件来实现他们的目标。
   - **发现**：伪装不仅是保守身份信息的行为，而且可以通过明确的误导来进行。伪装被视为一种理智的行动，而不仅仅是LLMs的生成幻觉。

**领导力**
   - **定义**：领导力涉及到影响其他玩家的行为，以控制游戏的进程。
   - **实例**：狼人可能建议其他人按照狼人的意图行动，或者呼吁先知揭露身份。
   - **发现**：呼吁采取行动和提供指导可能有助于获得其他玩家的支持。这种努力影响他人的行为显示了LLMs引人入胜的社交属性。


## 6. 缺陷总结

(1) **幻觉问题**：幻觉在生成内容中影响事实性，可能对模型的推理能力造成负面影响。
(2) **经验利用的局限**：尽管历史经验被视为提高模型能力的一种方式，但如何减轻噪声的不利影响和跨游戏的通用经验利用仍有待探讨。
(3) **与人类玩家的交互**：本研究未包含结合人类玩家得出的经验池，未来研究应探讨这一方面的可能性和效果。
(4) **游戏技巧的提高**：如何使LLM掌握更高级的游戏技巧和自主探索，例如教导人类玩家的经验，是一个吸引人的研究方向。
(5) **评估多LLMs设置的能力**：需要进一步探讨如何构建一个不变的基线来评估多LLMs设置的能力。  


## 7. 优化思路总结

### 7.1 问题总结与初步想法

**问题**：当前记忆库构建方式较简单，使用阈值存储多轮次记忆可能引入噪声，影响结果。
**解决方案**：模拟人类记忆习惯，实现记忆的动态管理和淡出效应，其中：
  - **近期记忆**更易保留；
  - **长期且频繁访问的记忆**也得以保留；
  - **久远或不常用的记忆**逐渐被淡忘。
**具体实现**：利用长短记忆网络的思想构建动态记忆库，或者用一个LLM代理作为记忆库的控制。

### 7.2 扩展思考

(1) **动态权重**：给不同时期的记忆分配不同的权重，这样我们可以让模型更关注近期的信息或经常被用到的信息，而逐渐淡忘老旧或不常用的信息。这种机制与人类的记忆模式相似，可以使模型的决策更加符合实际情况。
(2) **淡忘机制**：实现一种“淡忘”机制，即优先考虑近期的经验，同时也保留一些长期的重要经验。这样可以保持记忆库的动态更新。

### 7.3 动态管理记忆

(1) **时间衰减**: 为记忆库中的每一条信息分配一个权重，它基于其时序位置递减。例如，最近的信息权重最大，而最旧的信息权重最小。
(2) **频率加权**: 如果某些信息在多个游戏回合中反复出现，它可以被赋予更高的权重，即使它是旧的信息。